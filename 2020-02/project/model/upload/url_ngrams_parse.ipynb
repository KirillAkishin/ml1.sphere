{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import codecs\n",
    "\n",
    "stop_words = np.array(pd.read_csv('stopwords_russian2translit.txt', encoding='utf-8', lineterminator='\\n')).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_url(url, stop_words, min_len_word=1):\n",
    "#   url?text#text -> url:\n",
    "    temp = re.search(r'.+\\?', url)\n",
    "    if temp != None:\n",
    "        url = temp.group(0)[:-1]\n",
    "    temp = re.search(r'.+#', url)\n",
    "    if temp != None:\n",
    "        url = temp.group(0)[:-1]\n",
    "    \n",
    "#   site_name, full_domen, last_domen\n",
    "    full_domen = re.search(r'\\..+', url).group(0)[1:]\n",
    "    if re.search(r'.+?/', full_domen) != None:\n",
    "        full_domen = re.search(r'.+?/', full_domen).group(0)[:-1]    \n",
    "    last_domen = full_domen.strip().split(sep='.')[-1]\n",
    "    site_name = re.search(r'.+?\\.', url).group(0)[:-1]\n",
    "    \n",
    "#   usefull_names = url.split() - domens - stop_words - set(word: len(word) < n)\n",
    "    domens = full_domen.strip().split(sep='.')\n",
    "    usefull_names = set(re.split('-|_|\\.|/',url))\n",
    "    usefull_names = usefull_names.difference(stop_words)\n",
    "    usefull_names = usefull_names.difference(domens)\n",
    "    usefull_names = [item for item in usefull_names if len(item) >= min_len_word]\n",
    "    usefull_names = ' '.join(list(usefull_names))\n",
    "\n",
    "        \n",
    "    return full_domen, last_domen, site_name, usefull_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_txt = 'new_urls.txt'\n",
    "path = 'content/'\n",
    "\n",
    "with open(output_txt, 'w') as out_f:\n",
    "    out_f.write('{}\\t{}\\n'.format('doc_id', 'usefull_names'))\n",
    "    for doc_id in range(1, 28027):\n",
    "        with codecs.open(path + str(doc_id) + '.dat', 'r', 'utf-8') as in_f:\n",
    "            url = in_f.readline().strip()\n",
    "            _, _, _, usefull_names = parse_url(url, stop_words, min_len_word=2)\n",
    "            out_f.write('{}\\t{}\\n'.format(doc_id, usefull_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_txt = 'ngrams.txt'\n",
    "un = np.array(pd.read_csv('new_urls.txt', sep='\\t', encoding='utf-8', lineterminator='\\n').usefull_names).ravel()\n",
    "# un = filter(lambda v: v==v, un)\n",
    "n = 4\n",
    "\n",
    "with open(output_txt, 'w') as out_f:\n",
    "    out_f.write('{}\\t{}\\n'.format('doc_id', 'ngrams'))\n",
    "    for doc_id, it in enumerate(un):\n",
    "        if not isinstance(it, np.str):\n",
    "            out_f.write(str(doc_id)+'\\t'+''+'\\n')    \n",
    "            continue\n",
    "        out_f.write(str(doc_id)+'\\t')    \n",
    "        ngr = ngrams(it, n)\n",
    "        for grams in ngr:\n",
    "            out_f.write(''.join(grams) + ',')\n",
    "        out_f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
