{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import codecs\n",
    "from nltk import ngrams \n",
    "\n",
    "stop_words = np.array(pd.read_csv('stopwords_russian2translit.txt', encoding='utf-8', lineterminator='\\n')).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_url(url, stop_words, min_len_word=1):\n",
    "#   url?text#text -> url:\n",
    "    temp = re.search(r'.+\\?', url)\n",
    "    if temp != None:\n",
    "        url = temp.group(0)[:-1]\n",
    "    temp = re.search(r'.+#', url)\n",
    "    if temp != None:\n",
    "        url = temp.group(0)[:-1]\n",
    "    \n",
    "#   site_name, full_domen, last_domen\n",
    "    full_domen = re.search(r'\\..+', url).group(0)[1:]\n",
    "    if re.search(r'.+?/', full_domen) != None:\n",
    "        full_domen = re.search(r'.+?/', full_domen).group(0)[:-1]    \n",
    "    last_domen = full_domen.strip().split(sep='.')[-1]\n",
    "    site_name = re.search(r'.+?\\.', url).group(0)[:-1]\n",
    "    \n",
    "#   usefull_names = url.split() - domens - stop_words - set(word: len(word) < n)\n",
    "    usefull_names = re.search(r'.+', url).group(0)\n",
    "    if re.search(r'.+?/', usefull_names) != None:\n",
    "        usefull_names = re.search(r'.+?/', usefull_names).group(0)[:-1]          \n",
    "    usefull_names = set(re.split('-|_|\\.|/', usefull_names))\n",
    "    usefull_names = usefull_names.difference(stop_words)\n",
    "    usefull_names = [item for item in usefull_names if len(item) >= min_len_word]\n",
    "    usefull_names = ' '.join(list(usefull_names))\n",
    "\n",
    "        \n",
    "    return full_domen, last_domen, site_name, usefull_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_txt = 'new_sitenames.txt'\n",
    "path = 'content/'\n",
    "\n",
    "with open(output_txt, 'w') as out_f:\n",
    "    out_f.write('{}\\t{}\\n'.format('doc_id', 'usefull_names'))\n",
    "    for doc_id in range(1, 28027):\n",
    "        with codecs.open(path + str(doc_id) + '.dat', 'r', 'utf-8') as in_f:\n",
    "            url = in_f.readline().strip()\n",
    "            _, _, _, usefull_names = parse_url(url, stop_words, min_len_word=0)\n",
    "            out_f.write('{}\\t{}\\n'.format(doc_id, usefull_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>usefull_names</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>zrenielib ru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>online ru perevesti</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>timecops biz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>proffi95 ru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>xn p1ai jtbaaldsgaoflxr4fyc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>rabotka ru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>beon vladkakashi ru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>ru doshkolnik</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>com chastnosti</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>pomogudengami ru</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   doc_id                 usefull_names\n",
       "0       1                  zrenielib ru\n",
       "1       2           online ru perevesti\n",
       "2       3                  timecops biz\n",
       "3       4                   proffi95 ru\n",
       "4       5   xn p1ai jtbaaldsgaoflxr4fyc\n",
       "5       6                    rabotka ru\n",
       "6       7           beon vladkakashi ru\n",
       "7       8                 ru doshkolnik\n",
       "8       9                com chastnosti\n",
       "9      10              pomogudengami ru"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ns = pd.read_csv('new_sitenames.txt', sep='\\t', encoding='utf-8', lineterminator='\\n')\n",
    "ns.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_txt = 'ngrams_sitenames.txt'\n",
    "un = np.array(pd.read_csv('new_sitenames.txt', sep='\\t', encoding='utf-8', lineterminator='\\n').usefull_names).ravel()\n",
    "# un = filter(lambda v: v==v, un)\n",
    "n = 3\n",
    "\n",
    "with open(output_txt, 'w') as out_f:\n",
    "    out_f.write('{}\\t{}\\n'.format('doc_id', 'ngrams'))\n",
    "    for doc_id, it in enumerate(un):\n",
    "        if not isinstance(it, np.str):\n",
    "            out_f.write(str(doc_id)+'\\t'+''+'\\n')    \n",
    "            continue\n",
    "        out_f.write(str(doc_id)+'\\t')    \n",
    "        ngr = ngrams(it, n)\n",
    "        for grams in ngr:\n",
    "            out_f.write(''.join(grams) + ',')\n",
    "        out_f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
